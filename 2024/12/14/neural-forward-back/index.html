<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>前向传播和反向传播 | 风雪围城</title>

  
  <meta name="author" content="YueGS">
  

  
  <meta name="description" content="图片来源
神经网络，是一种模拟人脑神经元网络的计算模型。它由多个神经元组成，每个神经元通过权重和偏移，对输入信号进行处理，最终输出结果。如上图示，每个神经元就像是一个仪表盘，它通过输入的值，经过处理，最终输出一个值。训练过程，就是在调整这些仪表盘的参数，使得输出尽可能接近目标输出。">
  

  
  
  <meta name="keywords" content="LLM,神经网络,前向传播,反向传播">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="前向传播和反向传播"/>

  <meta property="og:site_name" content="风雪围城"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="风雪围城" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">风雪围城</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
        <li><a href="/categories">Categories</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/about">About</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>前向传播和反向传播</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2024/12/14/neural-forward-back/" rel="bookmark">
        <time class="entry-date published" datetime="2024-12-14T14:14:18.551Z">
          2024-12-14
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/image.png" alt="alt text"><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=wjZofJX0v4M&t=1351s">图片来源</a></p>
<p>神经网络，是一种模拟人脑神经元网络的计算模型。它由多个神经元组成，每个神经元通过权重和偏移，对输入信号进行处理，最终输出结果。<br>如上图示，每个神经元就像是一个仪表盘，它通过输入的值，经过处理，最终输出一个值。训练过程，就是在调整这些仪表盘的参数，使得输出尽可能接近目标输出。</p>
<span id="more"></span>

<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>如前图所示，前向传播的过程就是从输入层到隐藏层再到输出层的过程。</p>
<h4 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h4><p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/20241211202143.gif" alt="alt text"><br>隐藏层由多层神经元矩阵组成。第一层矩阵中的每个神经元与输入层相连，最后一层矩阵中的每个神经元与输出层相连，中间每层矩阵中的每个神经元都与上一层矩阵中的神经元相连，构成所谓的 <strong>全连接</strong>。<br>每个神经元中，我们可以使用一个简单的线性函数来表示神经元的输出，即：<code>y = wx + b</code>。这种函数的好处在于其描述的关系足够简单，计算的效率会很高。但是，它也有明显的缺点，即：无法描述复杂的非线性关系。<br>这种情况下，这种拟合无论使用多少层神经元，都只不过是单纯的矩阵运算，无法描述复杂的非线性关系，因此，我们需要引入激活函数来描述这种非线性关系。</p>
<h4 id="权重和偏移"><a href="#权重和偏移" class="headerlink" title="权重和偏移"></a>权重和偏移</h4><p><code>y = wx + b</code> 这个公式中，<code>w</code> 是权重，<code>b</code> 是偏移。<br>权重，说明神经元对输入的敏感程度，权重越大，说明神经元对输入的敏感程度越高。它放大了输入的值。<br>偏移，反映了神经元的基础激活偏向。偏移越大，更容易激活神经元。<br>权重和偏移的初始值，通常是随机生成的，因此，我们需要通过训练来调整这些参数，使得神经网络的输出尽可能接近目标输出。显而易见的，权重和偏移越大，y 的值越大。  </p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>激活函数是一个决策规则，描述了对输入信号做出反应的过程。它决定了神经元的新号是否会下一个神经元传递或者传递多少。<br>常见的激活函数比如 <code>ReLU</code>、<code>sigmoid</code>（一个更温和的决策者） 等。<br><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/relu-sigmoid.png"><br>对于 <code>ReLU</code> 来说，如果输入的值大于 0，则输出输入的值，否则输出 0。<br>对于 <code>sigmoid</code> 来说，如果输入的值大于 0，则输出 1，否则输出 0。<br>可以看到，实际上 <strong>激活函数引入了非线性关系，使得神经网络可以描述复杂的非线性关系</strong>。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/20241211202229.gif"><br>在前向传播中，输入通过层层神经元矩阵，最终得到输出。但是，这通常会存在很大的误差。我们需要通过反馈调节，调整各个神经元中的参数，以期望误差尽可能小。</p>
<p>反向传播的过程，在层层神经元矩阵中，从输出层开始，逐层向前，计算每个神经元对误差的贡献，并根据贡献调整权重和偏移，以期望误差尽可能小。</p>
<p>这其中，有两个问题需要被解决：</p>
<ul>
<li>参数该朝哪个方向调整</li>
<li>参数该调整多少</li>
</ul>
<p>这也就引入了新的概念：损失函数。我们定义一个损失函数，来量化误差，并通过损失函数，我们可以计算出参数的调整方向和调整量。</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>损失函数，描述了输出与目标之间的误差。常见的损失函数比如 <code>均方误差</code>（Mean Squared Error, MSE）、<code>交叉熵</code>（Cross Entropy, CE） 等。</p>
<h5 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h5><p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/mse.png"><br>公式如上，我们可以看到，其优势包括：</p>
<ul>
<li>计算简单，易于理解</li>
<li>曲线光滑，便于求导</li>
<li>凸函数，便于找到最优梯度</li>
<li>对大误差更敏感<br>它也存在着诸多缺点，比如，对小误差不敏感，对大误差更敏感；假设误差分布呈正态分布等。所以，最终选择哪种损失函数，需要根据实际情况来决定，MSE 通常用于回归问题（拟合输入特征和连续输出之间的映射关系）。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/20241215090746.gif"></p>
<p>我们可以看到，实际上，输出和目标值之间直接的误差只发生在最后一层，也就是输出层。输出层的误差会通过反向链式法则，逐层向上影响。造成最终误差的结果，是所有层误差的累加。</p>
<p>最后一层的 MSE 公式如下：<br><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/mse-func.png"></p>
<p><code>x</code> 是输入，<code>w</code> 是权重，<code>b</code> 是偏移。我们要调整的是 <code>w</code> 和 <code>b</code>。这里，<code>w</code> 和 <code>b</code> 的调整方向，是使得 <code>y</code> 尽可能接近 <code>target</code>。所以，<code>w</code> 和 <code>b</code> 才是这里的变量。最终，这个最小误差问题，就转换成了求解这个公式中，能够让 <code>MSE</code> 最小的 <code>w</code> 和 <code>b</code>。这两个变量是通过梯度下降法来求解的。</p>
<h5 id="梯度及参数更新"><a href="#梯度及参数更新" class="headerlink" title="梯度及参数更新"></a>梯度及参数更新</h5><p>梯度是个向量，方向指向函数值增加最快的方向，大小表示函数变化的速率。对于上述的 MSE 公式，我们可以求出其梯度，即：</p>
<p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/wb-gradient.png"></p>
<p>在隐藏层中，误差无法直接传播，因此，我们需要通过链式法则，将误差传递到隐藏层。最终的误差，隐藏层也负有责任。<br>同样，我们假设激活函数是线性的，来考虑误差是如何在隐藏层中传递。</p>
<p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/hidden-layer-mse.png"></p>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/forward-back/train_process.png" alt="train-process"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># sigmoid激活函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_derivative</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># sigmoid函数的导数</span></span><br><span class="line">    <span class="keyword">return</span> x * (<span class="number">1</span> - x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line">input_value = <span class="number">0.5</span>  <span class="comment"># 输入值</span></span><br><span class="line">weight1 = <span class="number">0.8</span>  <span class="comment"># 第一层权重</span></span><br><span class="line">weight2 = <span class="number">0.6</span>  <span class="comment"># 第二层权重</span></span><br><span class="line">target = <span class="number">0.9</span>  <span class="comment"># 目标输出值</span></span><br><span class="line">learning_rate = <span class="number">0.1</span>  <span class="comment"># 学习率</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练开始！&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;初始输入: <span class="subst">&#123;input_value&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;目标输出: <span class="subst">&#123;target&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行3轮训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    layer1 = sigmoid(input_value * weight1)  <span class="comment"># 隐藏层输出</span></span><br><span class="line">    output = sigmoid(layer1 * weight2)  <span class="comment"># 最终输出</span></span><br><span class="line">    error = target - output  <span class="comment"># 计算误差</span></span><br><span class="line"></span><br><span class="line">    gradient_output = layer1 * error * sigmoid_derivative(output)</span><br><span class="line">    error_at_hidden = gradient_output * weight2</span><br><span class="line">    gradient_hidden = input_value * error_at_hidden * sigmoid_derivative(layer1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新权重</span></span><br><span class="line">    weight2 = weight2 + gradient_output * learning_rate</span><br><span class="line">    weight1 = weight1 + gradient_hidden * learning_rate</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;第<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>轮训练:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;预测输出: <span class="subst">&#123;output:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;当前误差: <span class="subst">&#123;error:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;weight1更新为: <span class="subst">&#123;weight1:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;weight2更新为: <span class="subst">&#123;weight2:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;--------------------&quot;</span>)</span><br></pre></td></tr></table></figure>


<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=wjZofJX0v4M&t=1351s">Transformers (how LLMs work) explained visually | DL5</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/OWTcGPHb4xGThaYsNcTn7w">【由浅到深】从神经网络原理、Transformer模型演进、到代码工程实现</a></p>
</li>
</ul>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/AI/">AI</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/LLM/">LLM</a><a href="/tags/神经网络/">神经网络</a><a href="/tags/前向传播/">前向传播</a><a href="/tags/反向传播/">反向传播</a>
    </span>
    

    </div>

    
  </div>
</article>

  









    </main>

    <footer class="site-footer">
  <p class="site-info">
    
    &copy; 2025 YueGS
    
  </p>
</footer>
    
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116967169-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-116967169-1');
</script>

    
  </div>
</div>
</body>
</html>