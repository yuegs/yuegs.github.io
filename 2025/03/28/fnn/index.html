<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>前馈神经网络：Transformer 模型的核心设计（三） | 风雪围城</title>

  
  <meta name="author" content="YueGS">
  

  
  <meta name="description" content="计算机世界需要用数学语言来描述和拟合现实世界的规律。某种程度上，这些规律本身就像是一个待我们去发现的复杂函数。
如下图示，我们可以将 Transformer 看做是一个函数黑盒，它实现了”下一个词”的预测，就像是学会了人类的表达。">
  

  
  
  <meta name="keywords" content="AI,Transformer,前馈神经网络,FNN">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="前馈神经网络：Transformer 模型的核心设计（三）"/>

  <meta property="og:site_name" content="风雪围城"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="风雪围城" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">风雪围城</a>
    </h1>
    <p class="site-description"></p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
        <li><a href="/categories">Categories</a></li>
      
        <li><a href="/tags">Tags</a></li>
      
        <li><a href="/about">About</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>前馈神经网络：Transformer 模型的核心设计（三）</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2025/03/28/fnn/" rel="bookmark">
        <time class="entry-date published" datetime="2025-03-27T17:12:43.396Z">
          2025-03-28
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/fnn/link.webp"></p>
<p>计算机世界需要用数学语言来描述和拟合现实世界的规律。某种程度上，这些规律本身就像是一个待我们去发现的复杂函数。</p>
<p>如下图示，我们可以将 Transformer 看做是一个函数黑盒，它实现了”下一个词”的预测，就像是学会了人类的表达。<br><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/fnn/ftransformer-predict.gif"></p>
<span id="more"></span>

<p>在 Transformer 内部，前馈神经网络是 Transformer 的重要组成部分。它也是一种强大的函数拟合工具。</p>
<p>通过构建非线性网络结构，它能够逼近现实世界中的复杂规律。<br>它就像是一个特征加工车间，通过两道工序处理信息：</p>
<ul>
<li>“特征展开”，为了便于”观察”，将输入信息投射到一个更大的空间，并通过ReLU激活函数突出重要特征，就像在显微镜下观察时调整焦距，让细节更加清晰；</li>
<li>“特征提炼”，将重要信息提取出来，形成最终的特征表示。这种设计让FFN能够学习到更丰富的特征转换关系。</li>
</ul>
<p>“提炼”出来的特征，使得模型能够精准地理解输入信息，并应用于”下一个词”的预测。</p>
<h3 id="前馈神经网络的数学表达"><a href="#前馈神经网络的数学表达" class="headerlink" title="前馈神经网络的数学表达"></a>前馈神经网络的数学表达</h3><p>以Transformer中的FFN为例，它可以表示为：</p>
<p><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/fnn/fnn.png"></p>
<p>其中：</p>
<ul>
<li>x 是输入向量</li>
<li>W1 是第一个线性变换的权重矩阵，将特征映射到高维空间</li>
<li>b1 是第一个变换的偏置向量</li>
<li>max(0, ·) 是ReLU激活函数，保留正值，将负值置零</li>
<li>W2 是第二个线性变换的权重矩阵，将特征映射回原始维度</li>
<li>b2 是第二个变换的偏置向量</li>
</ul>
<p>从这个公式的结构可以清晰地识别出FFN的两个关键阶段：</p>
<ol>
<li><p><strong>扩展阶段</strong>：xW1 + b1 </p>
<ul>
<li>这一步中，W1 通常是一个形状为 [dmodel, dff] 的矩阵，其中 dff 远大于 dmodel</li>
<li>例如，在原始Transformer中，dmodel &#x3D; 512，而 dff &#x3D; 2048，是模型维度的4倍</li>
<li>这种设计使得特征能够投射到更高维的空间，便于捕捉更复杂的特征关系</li>
</ul>
</li>
<li><p><strong>压缩阶段</strong>：(…)W2 + b2</p>
<ul>
<li>在ReLU激活函数处理后，W2 的形状为 [dff, dmodel]，将特征空间压缩回原始维度</li>
<li>这种压缩不是简单的降维，而是一种信息提炼，保留经过非线性变换后最有价值的信息</li>
<li>可以理解为模型在”决定”哪些高维特征是解决当前任务最重要的</li>
</ul>
</li>
</ol>
<p>这种”扩展-压缩”的设计遵循了”表征空间转换”的原理：在更高维的空间中，原本难以区分的特征可能变得更容易分离，从而使模型能够学习到更复杂的模式。</p>
<p>一个实际的矩阵计算是：<br><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/fnn/fnn-formula1.webp"><br><img src="https://raw.githubusercontent.com/yuegs/yuegs.github.io/refs/heads/master/images/ai/fnn/fnn-formula2.webp"></p>
<p>在实际应用中，会使用多个前馈神经网络堆叠，多尺度的进行特征提取。</p>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p><a target="_blank" rel="noopener" href="https://medium.com/@sumith.madupu123/understanding-transformer-architecture-using-simple-math-be6c2e1cdcc7">Understanding Transformer Architecture Using Simple Math</a><br><a href="https://yuegs.com/2025/02/20/resnet/">ResNet 残差神经网络</a><br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/665731716/answer/3611802917">为什么transformer的FFN需要先升维再降维？</a></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/AI/">AI</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/AI/">AI</a><a href="/tags/Transformer/">Transformer</a><a href="/tags/前馈神经网络/">前馈神经网络</a><a href="/tags/FNN/">FNN</a>
    </span>
    

    </div>

    
  </div>
</article>

  









    </main>

    <footer class="site-footer">
  <p class="site-info">
    
    &copy; 2025 YueGS
    
  </p>
</footer>
    
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116967169-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-116967169-1');
</script>

    
  </div>
</div>
</body>
</html>